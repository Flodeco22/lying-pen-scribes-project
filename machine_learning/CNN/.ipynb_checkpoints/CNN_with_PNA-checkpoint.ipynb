{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "63538798-507b-476d-9ac6-ff53757f8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "3c2c6a42-59be-4749-a0a9-9b5ba7b5e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_file, labels_file, transform=None):\n",
    "        self.images = self.load_images(images_file)\n",
    "        self.labels = self.load_labels(labels_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def load_images(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)  # Skip first 16 bytes\n",
    "        num_images = len(data) // (28 * 28)\n",
    "        data = data.reshape(num_images, 28, 28)\n",
    "        return data.copy()  # Ensure the array is writable\n",
    "\n",
    "    def load_labels(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8, offset=8)  # Skip first 8 bytes\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "5e90cb72-05ee-4af2-b7da-f6e025f47ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_set = torchvision.datasets.MNIST(root='./data', train=True,\\n                                        download=True, transform=transform)\\nvalidation_set = torchvision.datasets.MNIST(root='./data', train=False,\\n                                        download=True, transform=transform)\\n\\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size,\\n                                         shuffle=False, num_workers=2)\\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\\n                                          shuffle=True, num_workers=2)\\n\""
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_file = \"dataset/train-images-idx3-ubyte\"\n",
    "train_labels_file = \"dataset/train-labels-idx1-ubyte\"\n",
    "validation_images_file = \"dataset/validation-images-idx3-ubyte\"\n",
    "validation_labels_file = \"dataset/validation-labels-idx1-ubyte\"\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "train_set = CustomDataset(train_images_file, train_labels_file, transform=transform)\n",
    "validation_set = CustomDataset(validation_images_file, validation_labels_file, transform=transform)\n",
    "\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "'''\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "validation_set = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "36f570c1-a909-4fbb-acf4-cb6f5115d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alef\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmap_dict = {0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", 5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"}\\nprint(map_dict[0])\\n'"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dictionnary_labels.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "map_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    key, value = line.split('_')\n",
    "    map_dict[int(key) - 1] = value\n",
    "\n",
    "print(map_dict[0])\n",
    "\n",
    "\"\"\"\n",
    "map_dict = {0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", 5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"}\n",
    "print(map_dict[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "2fca3b1e-cc26-4b4e-ade0-2bc6df303ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamed Yod   Hey   Shin \n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{map_dict[int(labels[j])]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "85384a0e-ff5e-42cc-9fc5-e7a4b2258962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 22)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "2be74d6b-a886-4ed1-aea4-aa9086d391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "ffba5c29-9a42-4ddf-bab9-7c97536ab3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.558\n",
      "[1,    12] loss: 17.272\n",
      "[1,    23] loss: 17.049\n",
      "[1,    34] loss: 16.974\n",
      "[1,    45] loss: 16.901\n",
      "[1,    56] loss: 16.751\n",
      "[1,    67] loss: 16.473\n",
      "[1,    78] loss: 16.419\n",
      "[1,    89] loss: 15.776\n",
      "[2,     1] loss: 1.374\n",
      "[2,    12] loss: 15.047\n",
      "[2,    23] loss: 14.304\n",
      "[2,    34] loss: 12.558\n",
      "[2,    45] loss: 11.913\n",
      "[2,    56] loss: 11.543\n",
      "[2,    67] loss: 11.262\n",
      "[2,    78] loss: 11.633\n",
      "[2,    89] loss: 10.931\n",
      "[3,     1] loss: 1.131\n",
      "[3,    12] loss: 11.561\n",
      "[3,    23] loss: 10.911\n",
      "[3,    34] loss: 10.739\n",
      "[3,    45] loss: 11.512\n",
      "[3,    56] loss: 11.215\n",
      "[3,    67] loss: 11.296\n",
      "[3,    78] loss: 10.943\n",
      "[3,    89] loss: 11.134\n",
      "[4,     1] loss: 1.138\n",
      "[4,    12] loss: 10.731\n",
      "[4,    23] loss: 10.312\n",
      "[4,    34] loss: 12.326\n",
      "[4,    45] loss: 11.183\n",
      "[4,    56] loss: 11.426\n",
      "[4,    67] loss: 11.127\n",
      "[4,    78] loss: 11.485\n",
      "[4,    89] loss: 11.324\n",
      "[5,     1] loss: 0.933\n",
      "[5,    12] loss: 10.861\n",
      "[5,    23] loss: 11.435\n",
      "[5,    34] loss: 11.037\n",
      "[5,    45] loss: 10.901\n",
      "[5,    56] loss: 11.190\n",
      "[5,    67] loss: 11.141\n",
      "[5,    78] loss: 10.912\n",
      "[5,    89] loss: 11.059\n",
      "[6,     1] loss: 0.875\n",
      "[6,    12] loss: 11.410\n",
      "[6,    23] loss: 11.165\n",
      "[6,    34] loss: 11.085\n",
      "[6,    45] loss: 11.211\n",
      "[6,    56] loss: 10.768\n",
      "[6,    67] loss: 11.070\n",
      "[6,    78] loss: 10.743\n",
      "[6,    89] loss: 10.805\n",
      "[7,     1] loss: 0.985\n",
      "[7,    12] loss: 10.695\n",
      "[7,    23] loss: 11.051\n",
      "[7,    34] loss: 10.971\n",
      "[7,    45] loss: 11.000\n",
      "[7,    56] loss: 11.253\n",
      "[7,    67] loss: 10.802\n",
      "[7,    78] loss: 10.719\n",
      "[7,    89] loss: 11.052\n",
      "[8,     1] loss: 1.101\n",
      "[8,    12] loss: 11.454\n",
      "[8,    23] loss: 10.890\n",
      "[8,    34] loss: 10.799\n",
      "[8,    45] loss: 11.261\n",
      "[8,    56] loss: 10.836\n",
      "[8,    67] loss: 10.792\n",
      "[8,    78] loss: 11.270\n",
      "[8,    89] loss: 10.784\n",
      "[9,     1] loss: 0.917\n",
      "[9,    12] loss: 10.627\n",
      "[9,    23] loss: 11.223\n",
      "[9,    34] loss: 11.089\n",
      "[9,    45] loss: 10.840\n",
      "[9,    56] loss: 11.183\n",
      "[9,    67] loss: 10.875\n",
      "[9,    78] loss: 10.754\n",
      "[9,    89] loss: 10.832\n",
      "[10,     1] loss: 0.921\n",
      "[10,    12] loss: 10.785\n",
      "[10,    23] loss: 10.826\n",
      "[10,    34] loss: 10.743\n",
      "[10,    45] loss: 11.232\n",
      "[10,    56] loss: 11.058\n",
      "[10,    67] loss: 11.376\n",
      "[10,    78] loss: 10.813\n",
      "[10,    89] loss: 11.206\n",
      "[11,     1] loss: 1.007\n",
      "[11,    12] loss: 10.993\n",
      "[11,    23] loss: 10.619\n",
      "[11,    34] loss: 11.065\n",
      "[11,    45] loss: 11.034\n",
      "[11,    56] loss: 10.896\n",
      "[11,    67] loss: 10.714\n",
      "[11,    78] loss: 11.088\n",
      "[11,    89] loss: 11.007\n",
      "[12,     1] loss: 0.868\n",
      "[12,    12] loss: 10.919\n",
      "[12,    23] loss: 10.656\n",
      "[12,    34] loss: 11.107\n",
      "[12,    45] loss: 10.975\n",
      "[12,    56] loss: 11.096\n",
      "[12,    67] loss: 10.733\n",
      "[12,    78] loss: 10.958\n",
      "[12,    89] loss: 10.871\n",
      "[13,     1] loss: 0.952\n",
      "[13,    12] loss: 10.978\n",
      "[13,    23] loss: 10.568\n",
      "[13,    34] loss: 10.741\n",
      "[13,    45] loss: 10.912\n",
      "[13,    56] loss: 11.135\n",
      "[13,    67] loss: 10.646\n",
      "[13,    78] loss: 10.684\n",
      "[13,    89] loss: 10.602\n",
      "[14,     1] loss: 1.033\n",
      "[14,    12] loss: 10.852\n",
      "[14,    23] loss: 10.645\n",
      "[14,    34] loss: 10.755\n",
      "[14,    45] loss: 10.861\n",
      "[14,    56] loss: 11.698\n",
      "[14,    67] loss: 10.875\n",
      "[14,    78] loss: 10.952\n",
      "[14,    89] loss: 10.779\n",
      "[15,     1] loss: 0.917\n",
      "[15,    12] loss: 10.929\n",
      "[15,    23] loss: 10.549\n",
      "[15,    34] loss: 10.959\n",
      "[15,    45] loss: 10.592\n",
      "[15,    56] loss: 10.688\n",
      "[15,    67] loss: 11.179\n",
      "[15,    78] loss: 10.947\n",
      "[15,    89] loss: 10.942\n",
      "[16,     1] loss: 0.891\n",
      "[16,    12] loss: 10.612\n",
      "[16,    23] loss: 10.764\n",
      "[16,    34] loss: 11.375\n",
      "[16,    45] loss: 11.022\n",
      "[16,    56] loss: 10.880\n",
      "[16,    67] loss: 10.914\n",
      "[16,    78] loss: 10.600\n",
      "[16,    89] loss: 10.808\n",
      "[17,     1] loss: 0.953\n",
      "[17,    12] loss: 11.106\n",
      "[17,    23] loss: 10.840\n",
      "[17,    34] loss: 10.412\n",
      "[17,    45] loss: 10.900\n",
      "[17,    56] loss: 10.467\n",
      "[17,    67] loss: 10.987\n",
      "[17,    78] loss: 10.840\n",
      "[17,    89] loss: 10.864\n",
      "[18,     1] loss: 1.049\n",
      "[18,    12] loss: 10.583\n",
      "[18,    23] loss: 10.568\n",
      "[18,    34] loss: 10.741\n",
      "[18,    45] loss: 11.223\n",
      "[18,    56] loss: 10.624\n",
      "[18,    67] loss: 10.722\n",
      "[18,    78] loss: 10.795\n",
      "[18,    89] loss: 10.857\n",
      "[19,     1] loss: 1.021\n",
      "[19,    12] loss: 10.852\n",
      "[19,    23] loss: 10.779\n",
      "[19,    34] loss: 10.757\n",
      "[19,    45] loss: 10.471\n",
      "[19,    56] loss: 10.827\n",
      "[19,    67] loss: 10.352\n",
      "[19,    78] loss: 10.750\n",
      "[19,    89] loss: 10.819\n",
      "[20,     1] loss: 0.956\n",
      "[20,    12] loss: 10.656\n",
      "[20,    23] loss: 10.598\n",
      "[20,    34] loss: 11.138\n",
      "[20,    45] loss: 10.942\n",
      "[20,    56] loss: 10.635\n",
      "[20,    67] loss: 10.799\n",
      "[20,    78] loss: 10.759\n",
      "[20,    89] loss: 10.183\n",
      "[21,     1] loss: 0.828\n",
      "[21,    12] loss: 11.016\n",
      "[21,    23] loss: 10.646\n",
      "[21,    34] loss: 10.878\n",
      "[21,    45] loss: 10.699\n",
      "[21,    56] loss: 10.844\n",
      "[21,    67] loss: 10.675\n",
      "[21,    78] loss: 10.318\n",
      "[21,    89] loss: 10.626\n",
      "[22,     1] loss: 0.834\n",
      "[22,    12] loss: 10.845\n",
      "[22,    23] loss: 10.072\n",
      "[22,    34] loss: 10.844\n",
      "[22,    45] loss: 10.755\n",
      "[22,    56] loss: 10.673\n",
      "[22,    67] loss: 11.282\n",
      "[22,    78] loss: 10.904\n",
      "[22,    89] loss: 10.597\n",
      "[23,     1] loss: 1.035\n",
      "[23,    12] loss: 10.531\n",
      "[23,    23] loss: 10.542\n",
      "[23,    34] loss: 10.411\n",
      "[23,    45] loss: 10.642\n",
      "[23,    56] loss: 11.068\n",
      "[23,    67] loss: 10.730\n",
      "[23,    78] loss: 10.780\n",
      "[23,    89] loss: 11.219\n",
      "[24,     1] loss: 1.002\n",
      "[24,    12] loss: 10.753\n",
      "[24,    23] loss: 10.601\n",
      "[24,    34] loss: 10.900\n",
      "[24,    45] loss: 10.637\n",
      "[24,    56] loss: 10.536\n",
      "[24,    67] loss: 10.860\n",
      "[24,    78] loss: 10.510\n",
      "[24,    89] loss: 11.092\n",
      "[25,     1] loss: 0.897\n",
      "[25,    12] loss: 10.499\n",
      "[25,    23] loss: 10.650\n",
      "[25,    34] loss: 11.213\n",
      "[25,    45] loss: 10.643\n",
      "[25,    56] loss: 10.630\n",
      "[25,    67] loss: 10.283\n",
      "[25,    78] loss: 10.677\n",
      "[25,    89] loss: 11.161\n",
      "[26,     1] loss: 0.959\n",
      "[26,    12] loss: 10.654\n",
      "[26,    23] loss: 10.819\n",
      "[26,    34] loss: 10.543\n",
      "[26,    45] loss: 10.257\n",
      "[26,    56] loss: 10.572\n",
      "[26,    67] loss: 10.820\n",
      "[26,    78] loss: 10.638\n",
      "[26,    89] loss: 10.769\n",
      "[27,     1] loss: 0.943\n",
      "[27,    12] loss: 10.575\n",
      "[27,    23] loss: 10.914\n",
      "[27,    34] loss: 10.489\n",
      "[27,    45] loss: 10.871\n",
      "[27,    56] loss: 10.962\n",
      "[27,    67] loss: 10.814\n",
      "[27,    78] loss: 10.823\n",
      "[27,    89] loss: 10.470\n",
      "[28,     1] loss: 0.945\n",
      "[28,    12] loss: 10.171\n",
      "[28,    23] loss: 10.910\n",
      "[28,    34] loss: 10.463\n",
      "[28,    45] loss: 10.925\n",
      "[28,    56] loss: 11.105\n",
      "[28,    67] loss: 10.859\n",
      "[28,    78] loss: 10.612\n",
      "[28,    89] loss: 10.612\n",
      "[29,     1] loss: 0.985\n",
      "[29,    12] loss: 10.545\n",
      "[29,    23] loss: 10.663\n",
      "[29,    34] loss: 10.617\n",
      "[29,    45] loss: 10.521\n",
      "[29,    56] loss: 10.515\n",
      "[29,    67] loss: 11.072\n",
      "[29,    78] loss: 10.477\n",
      "[29,    89] loss: 10.473\n",
      "[30,     1] loss: 0.890\n",
      "[30,    12] loss: 10.575\n",
      "[30,    23] loss: 10.280\n",
      "[30,    34] loss: 10.882\n",
      "[30,    45] loss: 10.744\n",
      "[30,    56] loss: 10.472\n",
      "[30,    67] loss: 10.441\n",
      "[30,    78] loss: 10.405\n",
      "[30,    89] loss: 10.723\n",
      "[31,     1] loss: 0.940\n",
      "[31,    12] loss: 10.609\n",
      "[31,    23] loss: 10.471\n",
      "[31,    34] loss: 10.599\n",
      "[31,    45] loss: 10.502\n",
      "[31,    56] loss: 10.614\n",
      "[31,    67] loss: 10.563\n",
      "[31,    78] loss: 10.804\n",
      "[31,    89] loss: 10.285\n",
      "[32,     1] loss: 0.836\n",
      "[32,    12] loss: 10.491\n",
      "[32,    23] loss: 10.072\n",
      "[32,    34] loss: 10.236\n",
      "[32,    45] loss: 10.467\n",
      "[32,    56] loss: 10.096\n",
      "[32,    67] loss: 10.654\n",
      "[32,    78] loss: 10.625\n",
      "[32,    89] loss: 10.331\n",
      "[33,     1] loss: 0.840\n",
      "[33,    12] loss: 10.341\n",
      "[33,    23] loss: 10.755\n",
      "[33,    34] loss: 10.288\n",
      "[33,    45] loss: 10.867\n",
      "[33,    56] loss: 10.612\n",
      "[33,    67] loss: 10.166\n",
      "[33,    78] loss: 10.228\n",
      "[33,    89] loss: 10.742\n",
      "[34,     1] loss: 0.854\n",
      "[34,    12] loss: 10.635\n",
      "[34,    23] loss: 10.600\n",
      "[34,    34] loss: 9.983\n",
      "[34,    45] loss: 10.392\n",
      "[34,    56] loss: 10.377\n",
      "[34,    67] loss: 10.532\n",
      "[34,    78] loss: 10.733\n",
      "[34,    89] loss: 11.098\n",
      "[35,     1] loss: 1.012\n",
      "[35,    12] loss: 10.166\n",
      "[35,    23] loss: 10.234\n",
      "[35,    34] loss: 10.396\n",
      "[35,    45] loss: 10.721\n",
      "[35,    56] loss: 10.301\n",
      "[35,    67] loss: 10.670\n",
      "[35,    78] loss: 10.144\n",
      "[35,    89] loss: 10.653\n",
      "[36,     1] loss: 0.941\n",
      "[36,    12] loss: 10.304\n",
      "[36,    23] loss: 10.402\n",
      "[36,    34] loss: 9.939\n",
      "[36,    45] loss: 10.233\n",
      "[36,    56] loss: 10.246\n",
      "[36,    67] loss: 10.253\n",
      "[36,    78] loss: 10.592\n",
      "[36,    89] loss: 10.412\n",
      "[37,     1] loss: 0.949\n",
      "[37,    12] loss: 10.173\n",
      "[37,    23] loss: 10.426\n",
      "[37,    34] loss: 10.756\n",
      "[37,    45] loss: 10.146\n",
      "[37,    56] loss: 10.060\n",
      "[37,    67] loss: 10.604\n",
      "[37,    78] loss: 10.051\n",
      "[37,    89] loss: 10.896\n",
      "[38,     1] loss: 0.897\n",
      "[38,    12] loss: 9.961\n",
      "[38,    23] loss: 10.169\n",
      "[38,    34] loss: 10.665\n",
      "[38,    45] loss: 10.553\n",
      "[38,    56] loss: 10.235\n",
      "[38,    67] loss: 10.087\n",
      "[38,    78] loss: 10.211\n",
      "[38,    89] loss: 10.324\n",
      "[39,     1] loss: 0.932\n",
      "[39,    12] loss: 9.955\n",
      "[39,    23] loss: 10.339\n",
      "[39,    34] loss: 10.357\n",
      "[39,    45] loss: 10.169\n",
      "[39,    56] loss: 10.328\n",
      "[39,    67] loss: 10.187\n",
      "[39,    78] loss: 10.413\n",
      "[39,    89] loss: 10.212\n",
      "[40,     1] loss: 0.899\n",
      "[40,    12] loss: 9.873\n",
      "[40,    23] loss: 10.347\n",
      "[40,    34] loss: 9.792\n",
      "[40,    45] loss: 10.394\n",
      "[40,    56] loss: 9.673\n",
      "[40,    67] loss: 10.463\n",
      "[40,    78] loss: 10.056\n",
      "[40,    89] loss: 10.816\n",
      "[41,     1] loss: 0.954\n",
      "[41,    12] loss: 10.098\n",
      "[41,    23] loss: 9.840\n",
      "[41,    34] loss: 10.189\n",
      "[41,    45] loss: 9.768\n",
      "[41,    56] loss: 10.215\n",
      "[41,    67] loss: 9.592\n",
      "[41,    78] loss: 10.176\n",
      "[41,    89] loss: 9.806\n",
      "[42,     1] loss: 1.132\n",
      "[42,    12] loss: 9.561\n",
      "[42,    23] loss: 10.155\n",
      "[42,    34] loss: 9.674\n",
      "[42,    45] loss: 10.366\n",
      "[42,    56] loss: 9.861\n",
      "[42,    67] loss: 9.335\n",
      "[42,    78] loss: 10.724\n",
      "[42,    89] loss: 9.780\n",
      "[43,     1] loss: 0.810\n",
      "[43,    12] loss: 9.763\n",
      "[43,    23] loss: 9.998\n",
      "[43,    34] loss: 9.413\n",
      "[43,    45] loss: 10.556\n",
      "[43,    56] loss: 9.358\n",
      "[43,    67] loss: 9.761\n",
      "[43,    78] loss: 9.821\n",
      "[43,    89] loss: 10.232\n",
      "[44,     1] loss: 0.866\n",
      "[44,    12] loss: 8.957\n",
      "[44,    23] loss: 9.884\n",
      "[44,    34] loss: 9.675\n",
      "[44,    45] loss: 9.730\n",
      "[44,    56] loss: 9.586\n",
      "[44,    67] loss: 10.297\n",
      "[44,    78] loss: 9.882\n",
      "[44,    89] loss: 10.552\n",
      "[45,     1] loss: 0.850\n",
      "[45,    12] loss: 9.385\n",
      "[45,    23] loss: 9.701\n",
      "[45,    34] loss: 9.747\n",
      "[45,    45] loss: 10.409\n",
      "[45,    56] loss: 9.917\n",
      "[45,    67] loss: 9.481\n",
      "[45,    78] loss: 9.704\n",
      "[45,    89] loss: 9.595\n",
      "[46,     1] loss: 0.706\n",
      "[46,    12] loss: 9.738\n",
      "[46,    23] loss: 9.794\n",
      "[46,    34] loss: 9.881\n",
      "[46,    45] loss: 10.037\n",
      "[46,    56] loss: 10.302\n",
      "[46,    67] loss: 9.019\n",
      "[46,    78] loss: 9.485\n",
      "[46,    89] loss: 9.200\n",
      "[47,     1] loss: 0.736\n",
      "[47,    12] loss: 9.456\n",
      "[47,    23] loss: 9.051\n",
      "[47,    34] loss: 9.794\n",
      "[47,    45] loss: 8.544\n",
      "[47,    56] loss: 9.376\n",
      "[47,    67] loss: 9.639\n",
      "[47,    78] loss: 9.747\n",
      "[47,    89] loss: 8.974\n",
      "[48,     1] loss: 0.709\n",
      "[48,    12] loss: 8.442\n",
      "[48,    23] loss: 8.711\n",
      "[48,    34] loss: 8.940\n",
      "[48,    45] loss: 10.175\n",
      "[48,    56] loss: 9.099\n",
      "[48,    67] loss: 9.673\n",
      "[48,    78] loss: 10.084\n",
      "[48,    89] loss: 9.021\n",
      "[49,     1] loss: 0.874\n",
      "[49,    12] loss: 9.189\n",
      "[49,    23] loss: 9.449\n",
      "[49,    34] loss: 7.720\n",
      "[49,    45] loss: 9.346\n",
      "[49,    56] loss: 8.989\n",
      "[49,    67] loss: 9.135\n",
      "[49,    78] loss: 10.201\n",
      "[49,    89] loss: 8.422\n",
      "[50,     1] loss: 0.569\n",
      "[50,    12] loss: 9.254\n",
      "[50,    23] loss: 9.246\n",
      "[50,    34] loss: 9.169\n",
      "[50,    45] loss: 9.461\n",
      "[50,    56] loss: 9.031\n",
      "[50,    67] loss: 7.808\n",
      "[50,    78] loss: 7.964\n",
      "[50,    89] loss: 9.231\n",
      "[51,     1] loss: 0.996\n",
      "[51,    12] loss: 8.523\n",
      "[51,    23] loss: 8.738\n",
      "[51,    34] loss: 8.399\n",
      "[51,    45] loss: 7.770\n",
      "[51,    56] loss: 9.301\n",
      "[51,    67] loss: 8.602\n",
      "[51,    78] loss: 8.532\n",
      "[51,    89] loss: 7.943\n",
      "[52,     1] loss: 0.893\n",
      "[52,    12] loss: 8.596\n",
      "[52,    23] loss: 8.073\n",
      "[52,    34] loss: 8.733\n",
      "[52,    45] loss: 7.983\n",
      "[52,    56] loss: 8.667\n",
      "[52,    67] loss: 7.585\n",
      "[52,    78] loss: 9.019\n",
      "[52,    89] loss: 9.071\n",
      "[53,     1] loss: 0.760\n",
      "[53,    12] loss: 7.764\n",
      "[53,    23] loss: 8.014\n",
      "[53,    34] loss: 7.480\n",
      "[53,    45] loss: 8.811\n",
      "[53,    56] loss: 8.187\n",
      "[53,    67] loss: 7.328\n",
      "[53,    78] loss: 8.426\n",
      "[53,    89] loss: 8.845\n",
      "[54,     1] loss: 0.845\n",
      "[54,    12] loss: 8.572\n",
      "[54,    23] loss: 6.505\n",
      "[54,    34] loss: 8.235\n",
      "[54,    45] loss: 8.446\n",
      "[54,    56] loss: 7.336\n",
      "[54,    67] loss: 7.241\n",
      "[54,    78] loss: 8.832\n",
      "[54,    89] loss: 7.884\n",
      "[55,     1] loss: 0.407\n",
      "[55,    12] loss: 7.221\n",
      "[55,    23] loss: 8.694\n",
      "[55,    34] loss: 7.958\n",
      "[55,    45] loss: 6.745\n",
      "[55,    56] loss: 7.172\n",
      "[55,    67] loss: 7.300\n",
      "[55,    78] loss: 7.048\n",
      "[55,    89] loss: 8.053\n",
      "[56,     1] loss: 0.431\n",
      "[56,    12] loss: 6.441\n",
      "[56,    23] loss: 7.465\n",
      "[56,    34] loss: 7.363\n",
      "[56,    45] loss: 7.204\n",
      "[56,    56] loss: 7.467\n",
      "[56,    67] loss: 6.968\n",
      "[56,    78] loss: 8.791\n",
      "[56,    89] loss: 6.911\n",
      "[57,     1] loss: 0.426\n",
      "[57,    12] loss: 6.543\n",
      "[57,    23] loss: 6.488\n",
      "[57,    34] loss: 7.409\n",
      "[57,    45] loss: 5.944\n",
      "[57,    56] loss: 7.008\n",
      "[57,    67] loss: 6.866\n",
      "[57,    78] loss: 6.255\n",
      "[57,    89] loss: 7.813\n",
      "[58,     1] loss: 0.403\n",
      "[58,    12] loss: 6.048\n",
      "[58,    23] loss: 6.295\n",
      "[58,    34] loss: 8.132\n",
      "[58,    45] loss: 5.622\n",
      "[58,    56] loss: 6.181\n",
      "[58,    67] loss: 6.050\n",
      "[58,    78] loss: 5.514\n",
      "[58,    89] loss: 7.555\n",
      "[59,     1] loss: 0.324\n",
      "[59,    12] loss: 4.978\n",
      "[59,    23] loss: 5.495\n",
      "[59,    34] loss: 5.559\n",
      "[59,    45] loss: 5.884\n",
      "[59,    56] loss: 5.710\n",
      "[59,    67] loss: 7.128\n",
      "[59,    78] loss: 6.259\n",
      "[59,    89] loss: 6.262\n",
      "[60,     1] loss: 0.667\n",
      "[60,    12] loss: 5.298\n",
      "[60,    23] loss: 4.492\n",
      "[60,    34] loss: 5.236\n",
      "[60,    45] loss: 5.863\n",
      "[60,    56] loss: 6.316\n",
      "[60,    67] loss: 6.136\n",
      "[60,    78] loss: 6.038\n",
      "[60,    89] loss: 5.508\n",
      "[61,     1] loss: 0.283\n",
      "[61,    12] loss: 3.663\n",
      "[61,    23] loss: 4.659\n",
      "[61,    34] loss: 5.417\n",
      "[61,    45] loss: 4.744\n",
      "[61,    56] loss: 5.523\n",
      "[61,    67] loss: 6.655\n",
      "[61,    78] loss: 5.820\n",
      "[61,    89] loss: 5.764\n",
      "[62,     1] loss: 0.297\n",
      "[62,    12] loss: 3.670\n",
      "[62,    23] loss: 4.171\n",
      "[62,    34] loss: 6.227\n",
      "[62,    45] loss: 4.778\n",
      "[62,    56] loss: 3.229\n",
      "[62,    67] loss: 4.850\n",
      "[62,    78] loss: 5.273\n",
      "[62,    89] loss: 5.193\n",
      "[63,     1] loss: 0.267\n",
      "[63,    12] loss: 4.127\n",
      "[63,    23] loss: 3.314\n",
      "[63,    34] loss: 4.397\n",
      "[63,    45] loss: 4.513\n",
      "[63,    56] loss: 4.107\n",
      "[63,    67] loss: 4.646\n",
      "[63,    78] loss: 4.569\n",
      "[63,    89] loss: 3.796\n",
      "[64,     1] loss: 0.192\n",
      "[64,    12] loss: 4.059\n",
      "[64,    23] loss: 4.196\n",
      "[64,    34] loss: 3.416\n",
      "[64,    45] loss: 3.657\n",
      "[64,    56] loss: 3.466\n",
      "[64,    67] loss: 3.712\n",
      "[64,    78] loss: 4.893\n",
      "[64,    89] loss: 4.789\n",
      "[65,     1] loss: 0.291\n",
      "[65,    12] loss: 2.690\n",
      "[65,    23] loss: 2.849\n",
      "[65,    34] loss: 2.154\n",
      "[65,    45] loss: 3.839\n",
      "[65,    56] loss: 3.711\n",
      "[65,    67] loss: 4.627\n",
      "[65,    78] loss: 4.680\n",
      "[65,    89] loss: 5.844\n",
      "[66,     1] loss: 0.292\n",
      "[66,    12] loss: 3.350\n",
      "[66,    23] loss: 3.034\n",
      "[66,    34] loss: 2.555\n",
      "[66,    45] loss: 3.045\n",
      "[66,    56] loss: 2.688\n",
      "[66,    67] loss: 2.846\n",
      "[66,    78] loss: 4.408\n",
      "[66,    89] loss: 4.398\n",
      "[67,     1] loss: 0.763\n",
      "[67,    12] loss: 4.576\n",
      "[67,    23] loss: 3.679\n",
      "[67,    34] loss: 3.319\n",
      "[67,    45] loss: 2.862\n",
      "[67,    56] loss: 3.606\n",
      "[67,    67] loss: 2.776\n",
      "[67,    78] loss: 3.267\n",
      "[67,    89] loss: 4.404\n",
      "[68,     1] loss: 0.059\n",
      "[68,    12] loss: 3.385\n",
      "[68,    23] loss: 3.274\n",
      "[68,    34] loss: 3.374\n",
      "[68,    45] loss: 2.498\n",
      "[68,    56] loss: 2.724\n",
      "[68,    67] loss: 2.325\n",
      "[68,    78] loss: 2.709\n",
      "[68,    89] loss: 4.019\n",
      "[69,     1] loss: 0.073\n",
      "[69,    12] loss: 1.862\n",
      "[69,    23] loss: 2.199\n",
      "[69,    34] loss: 2.059\n",
      "[69,    45] loss: 2.188\n",
      "[69,    56] loss: 3.259\n",
      "[69,    67] loss: 2.675\n",
      "[69,    78] loss: 2.590\n",
      "[69,    89] loss: 2.094\n",
      "[70,     1] loss: 0.042\n",
      "[70,    12] loss: 1.604\n",
      "[70,    23] loss: 1.683\n",
      "[70,    34] loss: 1.551\n",
      "[70,    45] loss: 1.222\n",
      "[70,    56] loss: 1.811\n",
      "[70,    67] loss: 2.385\n",
      "[70,    78] loss: 0.949\n",
      "[70,    89] loss: 2.219\n",
      "[71,     1] loss: 0.028\n",
      "[71,    12] loss: 1.159\n",
      "[71,    23] loss: 1.273\n",
      "[71,    34] loss: 1.318\n",
      "[71,    45] loss: 0.775\n",
      "[71,    56] loss: 1.319\n",
      "[71,    67] loss: 1.477\n",
      "[71,    78] loss: 2.391\n",
      "[71,    89] loss: 1.945\n",
      "[72,     1] loss: 0.550\n",
      "[72,    12] loss: 1.463\n",
      "[72,    23] loss: 2.875\n",
      "[72,    34] loss: 1.689\n",
      "[72,    45] loss: 1.751\n",
      "[72,    56] loss: 2.403\n",
      "[72,    67] loss: 1.784\n",
      "[72,    78] loss: 2.174\n",
      "[72,    89] loss: 1.980\n",
      "[73,     1] loss: 0.041\n",
      "[73,    12] loss: 1.724\n",
      "[73,    23] loss: 1.313\n",
      "[73,    34] loss: 0.850\n",
      "[73,    45] loss: 1.316\n",
      "[73,    56] loss: 1.775\n",
      "[73,    67] loss: 0.712\n",
      "[73,    78] loss: 0.997\n",
      "[73,    89] loss: 1.537\n",
      "[74,     1] loss: 0.019\n",
      "[74,    12] loss: 1.216\n",
      "[74,    23] loss: 1.476\n",
      "[74,    34] loss: 0.991\n",
      "[74,    45] loss: 1.276\n",
      "[74,    56] loss: 1.383\n",
      "[74,    67] loss: 1.110\n",
      "[74,    78] loss: 2.447\n",
      "[74,    89] loss: 2.545\n",
      "[75,     1] loss: 0.291\n",
      "[75,    12] loss: 1.633\n",
      "[75,    23] loss: 1.846\n",
      "[75,    34] loss: 2.389\n",
      "[75,    45] loss: 1.418\n",
      "[75,    56] loss: 1.283\n",
      "[75,    67] loss: 1.881\n",
      "[75,    78] loss: 1.302\n",
      "[75,    89] loss: 1.522\n",
      "[76,     1] loss: 0.046\n",
      "[76,    12] loss: 1.103\n",
      "[76,    23] loss: 0.540\n",
      "[76,    34] loss: 0.661\n",
      "[76,    45] loss: 0.506\n",
      "[76,    56] loss: 0.607\n",
      "[76,    67] loss: 0.478\n",
      "[76,    78] loss: 0.581\n",
      "[76,    89] loss: 0.893\n",
      "[77,     1] loss: 0.031\n",
      "[77,    12] loss: 1.143\n",
      "[77,    23] loss: 2.378\n",
      "[77,    34] loss: 1.663\n",
      "[77,    45] loss: 1.481\n",
      "[77,    56] loss: 1.337\n",
      "[77,    67] loss: 1.447\n",
      "[77,    78] loss: 1.209\n",
      "[77,    89] loss: 1.339\n",
      "[78,     1] loss: 0.020\n",
      "[78,    12] loss: 0.771\n",
      "[78,    23] loss: 0.499\n",
      "[78,    34] loss: 0.663\n",
      "[78,    45] loss: 0.996\n",
      "[78,    56] loss: 0.546\n",
      "[78,    67] loss: 1.079\n",
      "[78,    78] loss: 0.790\n",
      "[78,    89] loss: 0.364\n",
      "[79,     1] loss: 0.012\n",
      "[79,    12] loss: 0.316\n",
      "[79,    23] loss: 0.989\n",
      "[79,    34] loss: 0.324\n",
      "[79,    45] loss: 0.553\n",
      "[79,    56] loss: 0.308\n",
      "[79,    67] loss: 0.293\n",
      "[79,    78] loss: 0.263\n",
      "[79,    89] loss: 0.396\n",
      "[80,     1] loss: 0.024\n",
      "[80,    12] loss: 0.341\n",
      "[80,    23] loss: 0.444\n",
      "[80,    34] loss: 0.171\n",
      "[80,    45] loss: 0.450\n",
      "[80,    56] loss: 0.467\n",
      "[80,    67] loss: 0.083\n",
      "[80,    78] loss: 0.403\n",
      "[80,    89] loss: 0.259\n",
      "[81,     1] loss: 0.008\n",
      "[81,    12] loss: 0.308\n",
      "[81,    23] loss: 0.283\n",
      "[81,    34] loss: 0.139\n",
      "[81,    45] loss: 0.430\n",
      "[81,    56] loss: 0.299\n",
      "[81,    67] loss: 0.345\n",
      "[81,    78] loss: 0.372\n",
      "[81,    89] loss: 0.219\n",
      "[82,     1] loss: 0.001\n",
      "[82,    12] loss: 0.391\n",
      "[82,    23] loss: 0.345\n",
      "[82,    34] loss: 0.615\n",
      "[82,    45] loss: 0.342\n",
      "[82,    56] loss: 0.100\n",
      "[82,    67] loss: 0.417\n",
      "[82,    78] loss: 0.386\n",
      "[82,    89] loss: 0.145\n",
      "[83,     1] loss: 0.003\n",
      "[83,    12] loss: 0.103\n",
      "[83,    23] loss: 0.217\n",
      "[83,    34] loss: 0.144\n",
      "[83,    45] loss: 0.186\n",
      "[83,    56] loss: 0.151\n",
      "[83,    67] loss: 0.115\n",
      "[83,    78] loss: 0.093\n",
      "[83,    89] loss: 0.128\n",
      "[84,     1] loss: 0.008\n",
      "[84,    12] loss: 0.114\n",
      "[84,    23] loss: 0.058\n",
      "[84,    34] loss: 0.041\n",
      "[84,    45] loss: 0.083\n",
      "[84,    56] loss: 0.057\n",
      "[84,    67] loss: 0.076\n",
      "[84,    78] loss: 0.120\n",
      "[84,    89] loss: 0.099\n",
      "[85,     1] loss: 0.001\n",
      "[85,    12] loss: 0.031\n",
      "[85,    23] loss: 0.067\n",
      "[85,    34] loss: 0.055\n",
      "[85,    45] loss: 0.167\n",
      "[85,    56] loss: 0.114\n",
      "[85,    67] loss: 0.057\n",
      "[85,    78] loss: 0.095\n",
      "[85,    89] loss: 0.064\n",
      "[86,     1] loss: 0.006\n",
      "[86,    12] loss: 0.018\n",
      "[86,    23] loss: 0.105\n",
      "[86,    34] loss: 0.161\n",
      "[86,    45] loss: 0.034\n",
      "[86,    56] loss: 0.034\n",
      "[86,    67] loss: 0.140\n",
      "[86,    78] loss: 0.277\n",
      "[86,    89] loss: 0.577\n",
      "[87,     1] loss: 0.005\n",
      "[87,    12] loss: 0.251\n",
      "[87,    23] loss: 0.234\n",
      "[87,    34] loss: 0.131\n",
      "[87,    45] loss: 0.038\n",
      "[87,    56] loss: 0.046\n",
      "[87,    67] loss: 0.053\n",
      "[87,    78] loss: 0.037\n",
      "[87,    89] loss: 0.065\n",
      "[88,     1] loss: 0.002\n",
      "[88,    12] loss: 0.017\n",
      "[88,    23] loss: 0.034\n",
      "[88,    34] loss: 0.086\n",
      "[88,    45] loss: 0.019\n",
      "[88,    56] loss: 0.052\n",
      "[88,    67] loss: 0.027\n",
      "[88,    78] loss: 0.100\n",
      "[88,    89] loss: 0.048\n",
      "[89,     1] loss: 0.001\n",
      "[89,    12] loss: 0.044\n",
      "[89,    23] loss: 0.015\n",
      "[89,    34] loss: 0.036\n",
      "[89,    45] loss: 0.022\n",
      "[89,    56] loss: 0.019\n",
      "[89,    67] loss: 0.034\n",
      "[89,    78] loss: 0.026\n",
      "[89,    89] loss: 0.049\n",
      "[90,     1] loss: 0.000\n",
      "[90,    12] loss: 0.023\n",
      "[90,    23] loss: 0.026\n",
      "[90,    34] loss: 0.054\n",
      "[90,    45] loss: 0.016\n",
      "[90,    56] loss: 0.012\n",
      "[90,    67] loss: 0.015\n",
      "[90,    78] loss: 0.036\n",
      "[90,    89] loss: 0.021\n",
      "[91,     1] loss: 0.000\n",
      "[91,    12] loss: 0.025\n",
      "[91,    23] loss: 0.012\n",
      "[91,    34] loss: 0.031\n",
      "[91,    45] loss: 0.017\n",
      "[91,    56] loss: 0.025\n",
      "[91,    67] loss: 0.009\n",
      "[91,    78] loss: 0.015\n",
      "[91,    89] loss: 0.029\n",
      "[92,     1] loss: 0.007\n",
      "[92,    12] loss: 0.022\n",
      "[92,    23] loss: 0.016\n",
      "[92,    34] loss: 0.025\n",
      "[92,    45] loss: 0.029\n",
      "[92,    56] loss: 0.015\n",
      "[92,    67] loss: 0.014\n",
      "[92,    78] loss: 0.011\n",
      "[92,    89] loss: 0.015\n",
      "[93,     1] loss: 0.003\n",
      "[93,    12] loss: 0.020\n",
      "[93,    23] loss: 0.014\n",
      "[93,    34] loss: 0.013\n",
      "[93,    45] loss: 0.014\n",
      "[93,    56] loss: 0.018\n",
      "[93,    67] loss: 0.019\n",
      "[93,    78] loss: 0.015\n",
      "[93,    89] loss: 0.020\n",
      "[94,     1] loss: 0.000\n",
      "[94,    12] loss: 0.014\n",
      "[94,    23] loss: 0.018\n",
      "[94,    34] loss: 0.019\n",
      "[94,    45] loss: 0.009\n",
      "[94,    56] loss: 0.016\n",
      "[94,    67] loss: 0.025\n",
      "[94,    78] loss: 0.011\n",
      "[94,    89] loss: 0.021\n",
      "[95,     1] loss: 0.001\n",
      "[95,    12] loss: 0.013\n",
      "[95,    23] loss: 0.015\n",
      "[95,    34] loss: 0.019\n",
      "[95,    45] loss: 0.010\n",
      "[95,    56] loss: 0.008\n",
      "[95,    67] loss: 0.017\n",
      "[95,    78] loss: 0.015\n",
      "[95,    89] loss: 0.018\n",
      "[96,     1] loss: 0.000\n",
      "[96,    12] loss: 0.013\n",
      "[96,    23] loss: 0.008\n",
      "[96,    34] loss: 0.012\n",
      "[96,    45] loss: 0.027\n",
      "[96,    56] loss: 0.017\n",
      "[96,    67] loss: 0.006\n",
      "[96,    78] loss: 0.008\n",
      "[96,    89] loss: 0.022\n",
      "[97,     1] loss: 0.001\n",
      "[97,    12] loss: 0.017\n",
      "[97,    23] loss: 0.012\n",
      "[97,    34] loss: 0.012\n",
      "[97,    45] loss: 0.009\n",
      "[97,    56] loss: 0.015\n",
      "[97,    67] loss: 0.009\n",
      "[97,    78] loss: 0.008\n",
      "[97,    89] loss: 0.019\n",
      "[98,     1] loss: 0.002\n",
      "[98,    12] loss: 0.017\n",
      "[98,    23] loss: 0.012\n",
      "[98,    34] loss: 0.010\n",
      "[98,    45] loss: 0.019\n",
      "[98,    56] loss: 0.005\n",
      "[98,    67] loss: 0.007\n",
      "[98,    78] loss: 0.008\n",
      "[98,    89] loss: 0.016\n",
      "[99,     1] loss: 0.000\n",
      "[99,    12] loss: 0.011\n",
      "[99,    23] loss: 0.011\n",
      "[99,    34] loss: 0.007\n",
      "[99,    45] loss: 0.008\n",
      "[99,    56] loss: 0.010\n",
      "[99,    67] loss: 0.014\n",
      "[99,    78] loss: 0.016\n",
      "[99,    89] loss: 0.016\n",
      "[100,     1] loss: 0.000\n",
      "[100,    12] loss: 0.011\n",
      "[100,    23] loss: 0.008\n",
      "[100,    34] loss: 0.008\n",
      "[100,    45] loss: 0.013\n",
      "[100,    56] loss: 0.015\n",
      "[100,    67] loss: 0.012\n",
      "[100,    78] loss: 0.012\n",
      "[100,    89] loss: 0.010\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 11 == 0:    # print every 2 mini-batches for this example\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "PATH = './PNA_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "118380ae-9f8b-4db7-bc30-9c642252dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yod   Alef  Shin  Lamed\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(validation_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join(f'{map_dict[int(labels[j])]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "af937860-882e-4d86-8b79-dee414871ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c2576678-1aaa-45d7-87bf-f8c0b4f784a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "a27552e2-f34d-4787-9fdb-d6bd2fb568ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  Yod   Shin  Shin  Shin \n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{map_dict[int(predicted[j])]:5s}'\n",
    "                              for j in range(batch_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
